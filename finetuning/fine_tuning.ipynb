{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVa0caPZlogN"
   },
   "source": [
    "# Fine-Tuning LLMs with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT5BjFcflZAh"
   },
   "source": [
    "## Step 1: Installing and importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GLXwJqbjtPho"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping peft as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping trl as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft==0.13.2\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting trl==0.12.0\n",
      "  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from peft==0.13.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from peft==0.13.2) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from peft==0.13.2) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from peft==0.13.2) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from peft==0.13.2) (2.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from peft==0.13.2) (4.67.1)\n",
      "Collecting safetensors (from peft==0.13.2)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting huggingface-hub>=0.17.0 (from peft==0.13.2)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting datasets>=2.21.0 (from trl==0.12.0)\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting rich (from trl==0.12.0)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting scipy (from bitsandbytes)\n",
      "  Downloading scipy-1.16.1-cp312-cp312-macosx_14_0_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: filelock in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft==0.13.2) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.17.0->peft==0.13.2)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading pandas-2.3.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (91 kB)\n",
      "Collecting xxhash (from datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.17.0->peft==0.13.2)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-macosx_10_13_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading multidict-6.6.4-cp312-cp312-macosx_10_13_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading propcache-0.3.2-cp312-cp312-macosx_10_13_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading yarl-1.20.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: networkx in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.2) (80.9.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from torch>=1.13.0->peft==0.13.2) (1.13.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.13.0->peft==0.13.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft==0.13.2) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from pandas->datasets>=2.21.0->trl==0.12.0) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.21.0->trl==0.12.0)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl==0.12.0) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl==0.12.0)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from rich->trl==0.12.0) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl==0.12.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "Downloading trl-0.12.0-py3-none-any.whl (310 kB)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.9-cp37-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading aiohttp-3.12.15-cp312-cp312-macosx_10_13_x86_64.whl (476 kB)\n",
      "Downloading multidict-6.6.4-cp312-cp312-macosx_10_13_x86_64.whl (45 kB)\n",
      "Downloading yarl-1.20.1-cp312-cp312-macosx_10_13_x86_64.whl (91 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-macosx_10_13_x86_64.whl (47 kB)\n",
      "Downloading propcache-0.3.2-cp312-cp312-macosx_10_13_x86_64.whl (43 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-macosx_12_0_x86_64.whl (32.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.7/32.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp312-cp312-macosx_10_13_x86_64.whl (289 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_10_12_x86_64.whl (454 kB)\n",
      "Downloading pandas-2.3.2-cp312-cp312-macosx_10_13_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading scipy-1.16.1-cp312-cp312-macosx_14_0_x86_64.whl (23.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-macosx_10_9_x86_64.whl (31 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, scipy, safetensors, regex, pyarrow, propcache, multidict, mdurl, hf-xet, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, markdown-it-py, huggingface-hub, bitsandbytes, aiosignal, tokenizers, rich, aiohttp, accelerate, transformers, peft, datasets, trl\n",
      "\u001b[2K  Attempting uninstall: fsspec[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/30\u001b[0m [pyarrow]egex]]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.08;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/30\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/30\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.038;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/30\u001b[0m [fsspec]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/30\u001b[0m [trl]\u001b[38;5;237m━\u001b[0m \u001b[32m29/30\u001b[0m [trl]sformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 bitsandbytes-0.42.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.9 huggingface-hub-0.34.4 markdown-it-py-4.0.0 mdurl-0.1.2 multidict-6.6.4 multiprocess-0.70.16 pandas-2.3.2 peft-0.13.2 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 regex-2025.9.1 rich-14.1.0 safetensors-0.6.2 scipy-1.16.1 tokenizers-0.22.0 transformers-4.56.1 trl-0.12.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall accelerate peft bitsandbytes transformers trl -y\n",
    "!pip install accelerate peft==0.13.2 bitsandbytes transformers trl==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eRZm_OAbs3qA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (0.34.4)\n",
      "Requirement already satisfied: filelock in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from huggingface_hub) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from requests->huggingface_hub) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nAMzy_0FtaUZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz3vMSzhs-P7"
   },
   "source": [
    "## Step 2: Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llama_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maboonaji/llama2finetune-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                                                   \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfloat16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                                                                                                                                          \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnf4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m llama_model.config.use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      5\u001b[39m llama_model.config.pretraining_tp = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/udemyai/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/udemyai/lib/python3.12/site-packages/transformers/modeling_utils.py:288\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    290\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/udemyai/lib/python3.12/site-packages/transformers/modeling_utils.py:5008\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4999\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename.endswith(\n\u001b[32m   5000\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5001\u001b[39m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors.index.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   5002\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   5003\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5004\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5005\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_explicit_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   5006\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m5008\u001b[39m hf_quantizer, config, dtype, device_map = \u001b[43mget_hf_quantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\n\u001b[32m   5010\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5012\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5013\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   5014\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5015\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/udemyai/lib/python3.12/site-packages/transformers/quantizers/auto.py:319\u001b[39m, in \u001b[36mget_hf_quantizer\u001b[39m\u001b[34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[39m\n\u001b[32m    316\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     dtype = hf_quantizer.update_dtype(dtype)\n\u001b[32m    327\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/udemyai/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:88\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m0.43.1\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     89\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m         )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[31mImportError\u001b[39m: The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1."
     ]
    }
   ],
   "source": [
    "llama_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = \"aboonaji/llama2finetune-v2\",\n",
    "                                                   quantization_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = getattr(torch, \"float16\"),\n",
    "                                                                                                                                                          bnb_4bit_quant_type = \"nf4\"))\n",
    "llama_model.config.use_cache = False\n",
    "llama_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6aWb1e7tNRS"
   },
   "source": [
    "## Step 3: Loading the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b222dae50beb48b9909f90df348b98d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24fd942a593460586d790b9966ecc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4758eef3bf0840efb7e9de08f90efa8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01884b0b9ac4e828e3842fcf7a32a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49f3a39a308413285199b4d7397da97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = \"aboonaji/llama2finetune-v2\", trust_remote_code = True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coUlIR-ytjiF"
   },
   "source": [
    "## Step 4: Setting the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(output_dir = \"./results\", per_device_train_batch_size = 4, max_steps = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RGLZtFwHQiZ"
   },
   "source": [
    "## Step 5: Creating the Supervised Fine-Tuning trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llama_sft_trainer = SFTTrainer(model = \u001b[43mllama_model\u001b[49m,\n\u001b[32m      2\u001b[39m                                args = training_arguments,\n\u001b[32m      3\u001b[39m                                train_dataset = load_dataset(path = \u001b[33m\"\u001b[39m\u001b[33maboonaji/wiki_medical_terms_llam2_format\u001b[39m\u001b[33m\"\u001b[39m, split = \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      4\u001b[39m                                tokenizer = llama_tokenizer,\n\u001b[32m      5\u001b[39m                                peft_config = LoraConfig(task_type = \u001b[33m\"\u001b[39m\u001b[33mCAUSAL_LM\u001b[39m\u001b[33m\"\u001b[39m, r = \u001b[32m64\u001b[39m, lora_alpha = \u001b[32m16\u001b[39m, lora_dropout = \u001b[32m0.1\u001b[39m),\n\u001b[32m      6\u001b[39m                                dataset_text_field = \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'llama_model' is not defined"
     ]
    }
   ],
   "source": [
    "llama_sft_trainer = SFTTrainer(model = llama_model,\n",
    "                               args = training_arguments,\n",
    "                               train_dataset = load_dataset(path = \"aboonaji/wiki_medical_terms_llam2_format\", split = \"train\"),\n",
    "                               tokenizer = llama_tokenizer,\n",
    "                               peft_config = LoraConfig(task_type = \"CAUSAL_LM\", r = 64, lora_alpha = 16, lora_dropout = 0.1),\n",
    "                               dataset_text_field = \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSF8SHFKt1xL"
   },
   "source": [
    "## Step 6: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: To run the training you first need to sign up here: https://wandb.ai/authorize?ref=models\n",
    "# And then you will directly find in the main page your API key which you will enter in the output below.\n",
    "llama_sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMPw6WU6vbjP"
   },
   "source": [
    "## Step 7: Chatting with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Please tell me about Bursitis\"\n",
    "text_generation_pipeline = pipeline(task = \"text-generation\", model = llama_model, tokenizer = llama_tokenizer, max_length = 500)\n",
    "model_answer = text_generation_pipeline(f\"<s>[INST] {user_prompt} [/INST]\")\n",
    "print(model_answer[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1QdRqUg52KXlZM6H95ALtkgsLhoUJbfqY",
     "timestamp": 1700598650361
    },
    {
     "file_id": "1yo2NTyRaTTr0JJ4TCIqPunY2M8Dw7tOh",
     "timestamp": 1699244453424
    },
    {
     "file_id": "1_49KfvLBoF85Isc4Gdoyw28Oxxdpv6Eo",
     "timestamp": 1693751472053
    },
    {
     "file_id": "1s1ku_N8OJPKBluQBrpcZ7AkfdAcZ_Q12",
     "timestamp": 1692693138968
    },
    {
     "file_id": "1578XncaDK8vPWB3kLgw410DQNCqnZVec",
     "timestamp": 1692609627275
    },
    {
     "file_id": "1R3-kNFWLKu7tkgOM-mD99mm2nGD5-9Lh",
     "timestamp": 1692597817214
    },
    {
     "file_id": "1WVSL4serOfoXztXlNQLaaUQUjeufz9SF",
     "timestamp": 1692590287814
    },
    {
     "file_id": "1gaFqTnMarV6oTFgtPtxVudyiiXSlC845",
     "timestamp": 1692542130771
    },
    {
     "file_id": "1Rem8wniY65tq9Z0W5AdDfhUtm-yFpVuX",
     "timestamp": 1692463431916
    },
    {
     "file_id": "1SZSeXt3LCYrzjCEfWLuUd1vZadLm4v01",
     "timestamp": 1692341216495
    },
    {
     "file_id": "1hbd_kGydA8pKAYuqlKUXMOw6YJEhv4Tz",
     "timestamp": 1691472982802
    },
    {
     "file_id": "1YoUr7SPQrbBW3ubPpofTIb-99EdQv5S0",
     "timestamp": 1691423558809
    },
    {
     "file_id": "1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd",
     "timestamp": 1691305989771
    }
   ]
  },
  "kernelspec": {
   "display_name": "udemyai",
   "language": "python",
   "name": "udemyai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
