{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f792c734-a26a-450f-8f66-34188196285e",
   "metadata": {},
   "source": [
    "# Deep Q-Learning for Lunar Landing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a74e8b-4b54-47a2-aa58-f5d807d898c0",
   "metadata": {},
   "source": [
    "## Part 0 - Installing the required packages and importing the libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06fa823-23c6-46e7-9e7b-551e82eafd2f",
   "metadata": {},
   "source": [
    "### Installing Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586cd1a1-6dee-4fe6-8c07-15c95b22f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (1.2.0)\n",
      "\u001b[33mWARNING: gymnasium 1.2.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (2.3.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: ale_py>=0.9 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n",
      "Requirement already satisfied: swig in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (4.3.1)\n",
      "Requirement already satisfied: wheel in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (0.45.1)\n",
      "Requirement already satisfied: setuptools in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (80.9.0)\n",
      "Requirement already satisfied: pip in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (25.2)\n",
      "Requirement already satisfied: pyproject-toml in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (0.1.0)\n",
      "Requirement already satisfied: setuptools>=75.4.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from pyproject-toml) (80.9.0)\n",
      "Requirement already satisfied: wheel>=0.45.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from pyproject-toml) (0.45.1)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from pyproject-toml) (2.11.7)\n",
      "Requirement already satisfied: packaging>=24.2 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from pyproject-toml) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from pydantic>=2.9.2->pyproject-toml) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from pydantic>=2.9.2->pyproject-toml) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from pydantic>=2.9.2->pyproject-toml) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from pydantic>=2.9.2->pyproject-toml) (0.4.1)\n",
      "Requirement already satisfied: Box2D in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (2.3.10)\n",
      "Requirement already satisfied: gym in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gym) (2.3.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: gymnasium[box2d] in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[box2d]) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages (from gymnasium[box2d]) (4.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "!pip install swig\n",
    "!pip install wheel setuptools pip --upgrade\n",
    "!pip install pyproject-toml\n",
    "!pip install Box2D gym\n",
    "!pip install \"gymnasium[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51374521-5a7f-4871-8ae2-2049c27de232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8d9366-c9aa-45aa-b8f5-d94a790defdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4e390-e293-4ba1-ad37-b2d3ad79e532",
   "metadata": {},
   "source": [
    "## Part 1 - Building the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7624014a-25dc-424d-992a-c4321d8a4a44",
   "metadata": {},
   "source": [
    "### Creating the architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79196d08-21e3-4ab3-a620-3b088b5d164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of env can be found here: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "  def __init__(self, state_size, action_size, seed = 42):\n",
    "    super(Network, self).__init__()\n",
    "    self.seed = torch.manual_seed(seed)\n",
    "    self.fc1 = nn.Linear(state_size, 64) #fc = full conection\n",
    "    self.fc2 = nn.Linear(64, 64)\n",
    "    self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "  def forward(self, state):\n",
    "      x = self.fc1(state)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc2(state)\n",
    "      x = F.relu(x)\n",
    "      x = self.fc3(state)\n",
    "      return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7bb13e-d61e-4cf5-8ace-e020b8692ba2",
   "metadata": {},
   "source": [
    "# Part 2 - Training the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be1159-5f9e-4aca-b169-a92c01e0312d",
   "metadata": {},
   "source": [
    "## Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9984677f-50e2-4446-bec2-ca003b9a92fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "State size:  8\n",
      "Number of actions:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gthrone/anaconda3/envs/udemyai/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('LunarLander-v3') # The Lunar Lander environment was upgraded to v3\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "number_actions = env.action_space.n\n",
    "print('State shape: ', state_shape)\n",
    "print('State size: ', state_size)\n",
    "print('Number of actions: ', number_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb722b2-75cc-46ad-8445-c06ffa304d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd658a2-5495-4bff-afed-51c56c05bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "minibatch_size = 100\n",
    "discount_factor = 0.99\n",
    "replay_buffer_size = int(1e5)\n",
    "interpolation_parameter = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937eb451-a163-4faa-9723-7e0adcf9e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Implementing Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34400c64-3499-451f-93e5-923fba72517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        experiences = random.sample(self.memory,k=batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        return states, next_states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b51bf0c-aaa3-4c00-98e4-a9c5e617b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing the DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7d4b60d-2db2-42d3-ac09-a4a34fb929a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,states_size, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.local_qnetwork = Network(state_size, action_size).to(self.device) # select actions\n",
    "        self.target_qnetwork = Network(state_size, action_size).to(self.device) # calculate target Q values used in training for local. Stabailizes learning\n",
    "        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayMemory(replay_buffer_size)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done): \n",
    "        self.memory.push((state, action, reward, next_state, done))\n",
    "        self.t_step = (self.t_step + 1) % 4 #resetting every 4 steps, ie learning every 4 steps\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory.memory) > minibatch_size:\n",
    "                experiences = self.memory.sample(100)\n",
    "                self.learn(experiences, discount_factor)\n",
    "\n",
    "    def act(self, state, epsilon = 0.): #take action values and return final action taken\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.local_qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.local_qnetwork(state)\n",
    "        self.local_qnetowrk.train()\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, discount_factor): #Let's agent learn by using samples from history\n",
    "        states, next_states, actions, rewards, dones = experiences\n",
    "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + (discount_factor * next_q_targets * (1- dones))\n",
    "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(self.local_qnetowrk, self.target_qnetwork, interpolation_parameter)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, interpolation_parameter): #prevent abrupt changes by layering in history by updating target with local\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()): #instance of network:\n",
    "            target_param.data.copy_(interpolation_paramater * local_param.data + (1.0 - interpolation_parameter) * target_param.data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f15eb2-db26-4045-8fd2-ac1869f466b8",
   "metadata": {},
   "source": [
    "## Initializing DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51e159b0-f2fc-40ff-979e-9788c198590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size, number_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6856d891-d0b1-4a45-9f9e-4a863854aad4",
   "metadata": {},
   "source": [
    "## Training DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3ea6e-c4e2-4aba-9a6b-74785d91d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_episodes = 2000 #max episodes to train\n",
    "maximum_number_timesteps_per_episode = 1000\n",
    "epsilon_starting_value = 1.0\n",
    "epsilon_ending_value = 0.01\n",
    "epsilon_decay_value = 0.995\n",
    "epsilon = epsilon_starting_value\n",
    "scores_on_100_episodes = deque(maxlen = 100)\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    for t in range(maximum_number_timesteps_per_episode):\n",
    "        action = agent.act(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        agent.step(state, action, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "    scores_on_100_episodes.append(score)\n",
    "    epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f5dfd-c22c-4aa9-9caf-cd97dcf72c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ae0280-00fb-415f-8721-9924421d2f7c",
   "metadata": {},
   "source": [
    "# Part 3 - Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "397230dd-79ba-43b5-8d5d-2619876d21cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     env.close()\n\u001b[32m     18\u001b[39m     imageio.mimsave(\u001b[33m'\u001b[39m\u001b[33mvideo.mp4\u001b[39m\u001b[33m'\u001b[39m, frames, fps=\u001b[32m30\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m show_video_of_model(\u001b[43magent\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mLunarLander-v3\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow_video\u001b[39m():\n\u001b[32m     23\u001b[39m     mp4list = glob.glob(\u001b[33m'\u001b[39m\u001b[33m*.mp4\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action.item())\n",
    "    env.close()\n",
    "    imageio.mimsave('video.mp4', frames, fps=30)\n",
    "\n",
    "show_video_of_model(agent, 'LunarLander-v3')\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7e7c9-697f-4126-97f6-d1b7bd70fcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f81df1-e27c-4abd-b03b-385b2bf8f9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8bec1-d0a0-4783-9f34-facb9d503f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df6b0b-a778-41f9-9694-813bc82c9aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7243e-e421-4cf0-9dec-16ef0b53f961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemyai",
   "language": "python",
   "name": "udemyai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
